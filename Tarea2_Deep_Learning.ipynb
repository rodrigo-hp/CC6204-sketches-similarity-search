{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eSeqSIqXPtjs"
   },
   "source": [
    "# Tarea 2: Clasificación y búsqueda por similitud de sketches usando redes convolucionales\n",
    "\n",
    "### Autor: Rodrigo Hernández"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WBUTheRbP7rC"
   },
   "source": [
    "## Parte 0: Importar librerías y preparar la descarga de datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "apjHhMndQDod"
   },
   "source": [
    "Se importan las librerías que serán utilizadas en la tarea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Du6iWqgkzysI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from skimage import io\n",
    "from sklearn import preprocessing\n",
    "import scipy.misc\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dyw7pp3QQP8J"
   },
   "source": [
    "Se crean directorios que serán utilizados para guardar los archivos generados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "4dJvzDFVWamx"
   },
   "outputs": [],
   "source": [
    "!mkdir /content/t2\n",
    "!mkdir /content/t2/raw_data\n",
    "!mkdir /content/t2/tfrecords\n",
    "!mkdir /content/t2/model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r-PHBEGwQd3K"
   },
   "source": [
    "Entramos a la carpeta en donde guardaremos todos los archivos .ndjson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "7qyMAdcxc-h-"
   },
   "outputs": [],
   "source": [
    "cd /content/t2/raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O0GJ-ZZQQi8N"
   },
   "source": [
    "Descargamos todo el dataset simplificado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "U5QhLY3DWf4i"
   },
   "outputs": [],
   "source": [
    "!gsutil -m cp gs://quickdraw_dataset/full/simplified/* /content/t2/raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-xCKJQivQtVk"
   },
   "source": [
    "## Parte 1: Preprocesamiento de los datos y generación de las imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N_P-bob2REBD"
   },
   "source": [
    "La siguiente función genera los strokes sobre una imagen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MBrGmAfEQcza"
   },
   "outputs": [],
   "source": [
    "def parse_line(ndjson_line):\n",
    "  # get the JSON\n",
    "  sample = json.loads(ndjson_line)\n",
    "  # get the label of the example\n",
    "  class_name = sample[\"word\"]\n",
    "  if not class_name:\n",
    "    print (\"Empty classname\")\n",
    "    return None, None\n",
    "  # get the strokes of the example\n",
    "  drawing_array = sample[\"drawing\"]\n",
    "  # create the canvas\n",
    "  img = np.zeros((256, 256), dtype=np.uint8)\n",
    "  img[:,:] = 255\n",
    "  # make the drawing\n",
    "  for stroke in drawing_array:\n",
    "    xa = -1\n",
    "    ya = -1 \n",
    "    for p in zip(stroke[0], stroke[1]):\n",
    "      x = p[0]\n",
    "      y = p[1]\n",
    "      if xa >= 0 and ya >= 0:\n",
    "        cv2.line(img, (xa,ya), (x,y), 0, 3)\n",
    "      xa = x\n",
    "      ya = y\n",
    "  \n",
    "  return img, class_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VQmnE9oURIDi"
   },
   "source": [
    "La siguiente función se encarga de tomar 100 archivos .ndjson al azar y tomar 1000 ejemplos para entrenamiento y 50 para prueba. Retorna un arreglo con todos los ejemplos de training y otro con todos los ejemplos de testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "6SMGoilM6ooI"
   },
   "outputs": [],
   "source": [
    "def extract_ndjson_files(path):\n",
    "  # list all files in dir\n",
    "  files = [f for f in os.listdir(path) if os.path.isfile(f)]\n",
    "\n",
    "  # select 100 files randomly \n",
    "  samples = np.random.choice(files, 100, replace=False)\n",
    "  training_examples = []\n",
    "  test_examples = []\n",
    "  # open each one of the ndjson files\n",
    "  for file in samples:\n",
    "    with open(file) as f:\n",
    "      # get all of the examples of the file\n",
    "      content = []\n",
    "      for line in f:\n",
    "        content.append(line.rstrip('\\n'))\n",
    "    random.shuffle(content)\n",
    "    # add new examples to lists\n",
    "    training_examples.extend(content[:1000])\n",
    "    test_examples.extend(content[1000:1050])\n",
    "    del content[:]\n",
    "      \n",
    "  return training_examples, test_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TFngmDBmRZjI"
   },
   "source": [
    "La siguiente función se encarga de recorrer cada ejemplo en los arreglos y generar los strokes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Fn5t-9UzbLbL"
   },
   "outputs": [],
   "source": [
    "def drawings_parser(training_examples, test_examples):\n",
    "  training_drawings = []\n",
    "  training_labels = []\n",
    "  test_drawings = []\n",
    "  test_labels = []\n",
    "  \n",
    "  # parse every example\n",
    "  for ex in training_examples:\n",
    "    # first the training set\n",
    "    single_drawing, single_label = parse_line(ex)\n",
    "    training_drawings.append(single_drawing)\n",
    "    training_labels.append(single_label)\n",
    "  del training_examples[:]\n",
    "  for ex in test_examples:\n",
    "    # then the test set\n",
    "    single_drawing, single_label = parse_line(ex)\n",
    "    test_drawings.append(single_drawing)\n",
    "    test_labels.append(single_label)\n",
    "  del test_examples[:]\n",
    "  # create the label encoder to transform the classes to numbers\n",
    "  encoder = preprocessing.LabelEncoder()\n",
    "  encoder.fit(test_labels)\n",
    "  training_classes = list(encoder.transform(training_labels))\n",
    "  test_classes = list(encoder.transform(test_labels))\n",
    "  \n",
    "  return training_drawings, training_classes, test_drawings, test_classes, encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "obBx2urnRf2T"
   },
   "source": [
    "Se crea un método para reducir el tamaño de todas las imagenes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "0Oqbklcw5BoT"
   },
   "outputs": [],
   "source": [
    "def resize_images(images):\n",
    "  for i in range(0,len(images)):\n",
    "    images[i] = cv2.resize(images[i], (128, 128), interpolation = cv2.INTER_AREA)\n",
    "  \n",
    "  return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UqitNDWkRoon"
   },
   "source": [
    "Se crea un método para aleatorizar los arreglos de imagenes y de clases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-aQWYvPpqIzF"
   },
   "outputs": [],
   "source": [
    "def shuffle_images(images, labels):\n",
    "  images = np.array(images)\n",
    "  labels = np.array(labels)\n",
    "  inds = list(range(len(labels)))\n",
    "  np.random.shuffle(inds)\n",
    "  images = images[inds]\n",
    "  labels = labels[inds]\n",
    "  return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S__0wWvDRujl"
   },
   "source": [
    "Se generan los ejemplos de training y los de prueba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "0Xg2s316NweK"
   },
   "outputs": [],
   "source": [
    "training_examples, test_examples = extract_ndjson_files('/content/t2/raw_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bJrfC8aiRyem"
   },
   "source": [
    "Se generan las imagenes en conjunto con sus clases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "kZo58IiUcKUG"
   },
   "outputs": [],
   "source": [
    "training_drawings, training_classes, test_drawings, test_classes, encoder = drawings_parser(training_examples, test_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QXX-7eMyR2sW"
   },
   "source": [
    "Se reduce el tamaño de todas las imagenes a 128x128:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "fNKjrgTZ4Bxu"
   },
   "outputs": [],
   "source": [
    "#del training_examples[:]\n",
    "#del test_examples[:]\n",
    "training_drawings = resize_images(training_drawings)\n",
    "test_drawings = resize_images(test_drawings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GVWSqc9pR6KP"
   },
   "source": [
    "Se plotea una imagen al azar y la clase a la que corresponde:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Kftu9IH0F5xr"
   },
   "outputs": [],
   "source": [
    "ex = 4500\n",
    "import matplotlib.pyplot as plt\n",
    "print(encoder.inverse_transform(test_classes[ex]))\n",
    "imgplot = plt.imshow(test_drawings[ex])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WeDEhNfESHC7"
   },
   "source": [
    "Se ordenan los arreglos al azar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "U_EU6MYYqkbU"
   },
   "outputs": [],
   "source": [
    "# shuffle both arrays\n",
    "training_drawings, training_classes = shuffle_images(training_drawings, training_classes)\n",
    "test_drawings, test_classes = shuffle_images(test_drawings, test_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1CKzZYzYSZcK"
   },
   "source": [
    "## Parte 2: Transformación de las imágenes a TFRecords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zYK3zKuVSLgH"
   },
   "source": [
    "Se definen funciones para generar los TFRercords a partir de las imagenes y clases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "lzSDFIEUrp-L"
   },
   "outputs": [],
   "source": [
    "# %% int64 should be used for integer numeric values\n",
    "def _int64_feature(value):\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "# %% byte should be used for string  | char data\n",
    "def _bytes_feature(value):\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "# %% float should be used for floating point data\n",
    "def _float_feature(value):    \n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def createTFRecord(images, labels, image_shape, tfr_filename):\n",
    "    h = image_shape[0]\n",
    "    w = image_shape[1]    \n",
    "    writer = tf.python_io.TFRecordWriter(tfr_filename)    \n",
    "    assert len(images) == len(labels)\n",
    "    mean_image = np.zeros([h,w], dtype=np.float32)\n",
    "    for i in range(len(images)):        \n",
    "        image = images[i,:,:]\n",
    "        #print(\"{}label: {}\".format(label[i]))\n",
    "        #create a feature\n",
    "        feature = {'train/label': _int64_feature(labels[i]), \n",
    "                   'train/image': _bytes_feature(tf.compat.as_bytes(image.tostring()))}\n",
    "        #crate an example protocol buffer\n",
    "        example = tf.train.Example(features = tf.train.Features(feature=feature))        \n",
    "        #serialize to string an write on the file\n",
    "        writer.write(example.SerializeToString())\n",
    "        mean_image = mean_image + images[i, :, :] / len(images)\n",
    "\n",
    "    mean_image = mean_image         \n",
    "    #serialize mean_image\n",
    "    writer.close()\n",
    "    sys.stdout.flush()\n",
    "    return mean_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "9Q40kB97l-p3"
   },
   "outputs": [],
   "source": [
    "def createSketchTFRecord(str_path, id_type, im_size, images, labels):    \n",
    "    image_shape = np.array([im_size, im_size])\n",
    "    number_of_classes = 0 # deprecated, this variable is kept por compatibility    \n",
    "    if ( id_type + 1 ) & 1 : # train\n",
    "        tfr_filename = os.path.join(str_path, \"train.tfrecords\") \n",
    "        mean_image = createTFRecord(images, labels, image_shape, tfr_filename)\n",
    "        #saving mean image        \n",
    "        print(\"train_record saved at {}.\".format(tfr_filename))        \n",
    "        mean_file = os.path.join(str_path, \"mean.dat\")\n",
    "        print(\"mean_file {}\".format(mean_image.shape))\n",
    "        mean_image.tofile(mean_file)\n",
    "        print(\"mean_file saved at {}.\".format(mean_file))                          \n",
    "    elif ( id_type + 1 ) & 2 : # test\n",
    "        tfr_filename = os.path.join(str_path, \"test.tfrecords\")\n",
    "        createTFRecord(images, labels, image_shape, tfr_filename)\n",
    "        print(\"test_record saved at {}.\".format(tfr_filename))\n",
    "    #saving metadata file    \n",
    "    metadata_array = np.append(image_shape, [number_of_classes])                        \n",
    "    metadata_file = os.path.join(str_path, \"metadata.dat\")\n",
    "    metadata_array.tofile(metadata_file)\n",
    "    print(\"metadata_file saved at {}.\".format(metadata_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "St0ScLCeSR9t"
   },
   "source": [
    "Se generan los TFRecords de las imagenes de entrenamiento y sus clases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "dDtQ_Ied4ljL"
   },
   "outputs": [],
   "source": [
    "createSketchTFRecord('/content/t2/tfrecords', 0, 128, training_drawings, training_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_o43P01JShM7"
   },
   "source": [
    "Se generan los TFRecords de las imagenes de prueba y sus clases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "qL1yce0U-6wS"
   },
   "outputs": [],
   "source": [
    "createSketchTFRecord('/content/t2/tfrecords', 1, 128, test_drawings, test_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZszrdB6YSuag"
   },
   "source": [
    "Se define una función para interpretar los TFRecords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "8GNViXgvDppO"
   },
   "outputs": [],
   "source": [
    "def parser_tfrecord_sk(serialized_example, im_size, mean_img, number_of_classes):    \n",
    "    features = tf.parse_example([serialized_example],\n",
    "                                features={\n",
    "                                        'train/image': tf.FixedLenFeature([], tf.string),\n",
    "                                        'train/label': tf.FixedLenFeature([], tf.int64),\n",
    "                                        })\n",
    "    image = tf.decode_raw(features['train/image'], tf.uint8)    \n",
    "    image = tf.reshape(image, [im_size, im_size])\n",
    "    image = tf.cast(image, tf.float32) - tf.cast(tf.constant(mean_img), tf.float32)\n",
    "    #image = image * 1.0 / 255.0    \n",
    "    #one-hot \n",
    "    label = tf.one_hot(tf.cast(features['train/label'], tf.int32), number_of_classes)\n",
    "    label = tf.reshape(label, [number_of_classes])\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ICwRBMtnSyPY"
   },
   "source": [
    "## Parte 3:\tDiseño de la arquitectura de las redes convolucionales "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JnfnIVtSS3mK"
   },
   "source": [
    "Se definen las capas que se utilizarán en los modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "BBaB4v-zb80j"
   },
   "outputs": [],
   "source": [
    "#gaussian weights \n",
    "def gaussian_weights(shape,  mean, stddev):\n",
    "    return tf.truncated_normal(shape, \n",
    "                               mean = mean, \n",
    "                               stddev = stddev)\n",
    "\n",
    "#fully-connected layer fc\n",
    "def fc_layer(input, size, name, use_relu=True): \n",
    "    layer_shape_in =  input.get_shape()\n",
    "    # shape is a 1D tensor with 4 values\n",
    "    num_features_in = layer_shape_in[1:4].num_elements()\n",
    "    #reshape to  1D vector\n",
    "    input_reshaped = tf.reshape(input, [-1, num_features_in])\n",
    "    shape = [num_features_in, size]\n",
    "    W = tf.Variable(gaussian_weights(shape, 0.0, 0.02), name=name)     \n",
    "    b = tf.Variable(tf.zeros(size))\n",
    "    #\n",
    "    layer = tf.add( tf.matmul(input_reshaped, W) ,  b)    \n",
    "    \n",
    "    if use_relu:\n",
    "        layer=tf.nn.relu(layer)\n",
    "    return  layer\n",
    "  \n",
    "#convolution layer using stride = 1, is_training is added to set BN approproiately    \n",
    "def conv_layer(input, shape, name, stride = 1, is_training = False):    \n",
    "    #weights are initialized according to a gaussian distribution\n",
    "    W =  tf.Variable(gaussian_weights(shape, 0.0, 0.01), name=name)     \n",
    "    #weights for bias ares fixed as constants 0\n",
    "    b = tf.Variable(tf.zeros(shape[3]), name='bias_'+name)\n",
    "    return tf.nn.relu(\n",
    "            tf.layers.batch_normalization(\n",
    "                tf.add(tf.nn.conv2d(\n",
    "                        input, \n",
    "                        W, \n",
    "                        strides=[1, stride, stride, 1], \n",
    "                        padding='SAME'), b), scale = True, training = is_training))\n",
    "\n",
    "#pooling layer that uses max_pool\n",
    "def max_pool_layer(input, kernel, stride):\n",
    "    return tf.nn.max_pool(input,  \n",
    "                          [1, kernel, kernel, 1], \n",
    "                          [1, stride, stride, 1], \n",
    "                          padding = 'SAME' )\n",
    "#dropout\n",
    "def dropout_layer(input, prob):\n",
    "    \"\"\"prob is a float representing the probability that each element is kept\"\"\"\n",
    "    return tf.nn.dropout(input, prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i9X4iqtoTSQO"
   },
   "source": [
    "# Es importante que desde ahora en adelante se ejecuten las celdas que corresponden a una red en particular, no mezclar las celdas de cada red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HazfAQeThcmn"
   },
   "source": [
    "## Definimos la arquitectura de la red skNet\n",
    "\n",
    "## Importante: solo ejecutar las celdas que correspondan a skNet desde aquí en adelante para no perjudicar la ejecución completa del notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "1MpYEM3uBBkx"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_steps = 200\n",
    "batch_size = 200\n",
    "dropout = 0.75\n",
    "\n",
    "def skNet(features, n_classes=100, is_training=True):\n",
    "  with tf.variable_scope(\"model_scope\"):\n",
    "    x_tensor = tf.reshape(features, shape=[-1, 128, 128, 1])\n",
    "\n",
    "    # First block of conv layers\n",
    "    conv1_1 = conv_layer(x_tensor, shape = [3, 3, 1, 64], name='conv1_1', is_training = is_training)\n",
    "    conv1_2 = conv_layer(conv1_1, shape = [3, 3, 64, 64], name='conv1_2', is_training = is_training)\n",
    "    conv1 = max_pool_layer(conv1_2, 3, 2)\n",
    "    \n",
    "    # Second block of conv layers\n",
    "    conv2_1 = conv_layer(conv1, shape = [3, 3, 64, 128], name='conv2_1', is_training = is_training)\n",
    "    conv2_2 = conv_layer(conv2_1, shape = [3, 3, 128, 128], name='conv2_2', is_training = is_training)\n",
    "    conv2 = max_pool_layer(conv2_2, 3, 2)\n",
    "    \n",
    "    # Third block of conv layers\n",
    "    conv3_1 = conv_layer(conv2, shape = [3, 3, 128, 128], name='conv3_1', is_training = is_training)\n",
    "    conv3_2 = conv_layer(conv3_1, shape = [3, 3, 128, 128], name='conv3_2', is_training = is_training)\n",
    "    conv3 = max_pool_layer(conv3_2, 3, 2)\n",
    "    \n",
    "    # Fourth block of conv layers\n",
    "    conv4_1 = conv_layer(conv3, shape = [3, 3, 128, 256], name='conv4_1', is_training = is_training)\n",
    "    conv4_2 = conv_layer(conv4_1, shape = [3, 3, 256, 256], name='conv4_2', is_training = is_training)\n",
    "    conv4 = max_pool_layer(conv4_2, 3, 2)\n",
    "\n",
    "    # First fully connected\n",
    "    fc_1 = fc_layer(conv4, 1024, name='fc_1')\n",
    "\n",
    "    # Apply Dropout\n",
    "    drop = dropout_layer(fc_1, dropout)\n",
    "\n",
    "    # Second fully connected\n",
    "    fc_2 = fc_layer(drop, 100, name='fc_2', use_relu = False)\n",
    "  \n",
    "  return {\"output\": fc_2, \"deep_feature\": fc_1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ykhiyKeNhfcm"
   },
   "source": [
    "## Definimos la arquitectura del modelo skResNet\n",
    "\n",
    "## Importante: solo ejecutar las celdas que correspondan a skResNet desde aquí en adelante para no perjudicar la ejecución completa del notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "LNtLnHfXAIei"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_steps = 200\n",
    "batch_size = 50\n",
    "dropout = 0.75\n",
    "\n",
    "def skResNet(features, n_classes=100, is_training=True):\n",
    "  with tf.variable_scope(\"model_scope\"):\n",
    "    x_tensor = tf.reshape(features, shape=[-1, 128, 128, 1])\n",
    "    \n",
    "    # First block of conv layers\n",
    "    conv1_1 = conv_layer(x_tensor, shape = [3, 3, 1, 64], name='conv1_1', is_training = is_training)\n",
    "    conv1_2 = conv_layer(conv1_1, shape = [3, 3, 64, 64], name='conv1_2', is_training = is_training)\n",
    "    maxpool_1 = max_pool_layer(conv1_2, 3, 2)\n",
    "    \n",
    "    # Second block of conv layers\n",
    "    conv2_1 = conv_layer(maxpool_1, shape = [3, 3, 64, 64], name='conv2_1', is_training = is_training)\n",
    "    conv2_2 = conv_layer(conv2_1, shape = [3, 3, 64, 64], name='conv2_2', is_training = is_training)\n",
    "    residual_1 = tf.add(conv2_2, maxpool_1, name='residual_1')\n",
    "    \n",
    "    # Third block of conv layers\n",
    "    conv3_1 = conv_layer(residual_1, shape = [3, 3, 64, 64], name='conv3_1', is_training = is_training)\n",
    "    conv3_2 = conv_layer(conv3_1, shape = [3, 3, 64, 64], name='conv3_2', is_training = is_training)\n",
    "    residual_2 = tf.add(conv3_2, residual_1, name='residual_2')\n",
    "    \n",
    "    # Fourth block of conv layers\n",
    "    conv4_1 = conv_layer(residual_2, shape = [3, 3, 64, 128], name='conv4_1', is_training = is_training)\n",
    "    maxpool_2 = max_pool_layer(conv4_1, 3, 2)\n",
    "    \n",
    "    # Fifth block of conv layers\n",
    "    conv5_1 = conv_layer(maxpool_2, shape = [3, 3, 128, 128], name='conv5_1', is_training = is_training)\n",
    "    conv5_2 = conv_layer(conv5_1, shape = [3, 3, 128, 128], name='conv5_2', is_training = is_training)\n",
    "    residual_3 = tf.add(conv5_2, maxpool_2, name='residual_3')\n",
    "    \n",
    "    # Sixth block of conv layers\n",
    "    conv6_1 = conv_layer(residual_3, shape = [3, 3, 128, 128], name='conv6_1', is_training = is_training)\n",
    "    conv6_2 = conv_layer(conv6_1, shape = [3, 3, 128, 128], name='conv6_2', is_training = is_training)\n",
    "    residual_4 = tf.add(conv6_2, residual_3, name='residual_4')\n",
    "    \n",
    "    # Seventh block of conv layers\n",
    "    conv7_1 = conv_layer(residual_4, shape = [3, 3, 128, 256], name='conv7_1', is_training = is_training)\n",
    "    maxpool_3 = max_pool_layer(conv7_1, 3, 2)\n",
    "    \n",
    "    # Eight block of conv layers\n",
    "    conv8_1 = conv_layer(maxpool_3, shape = [3, 3, 256, 256], name='conv8_1', is_training = is_training)\n",
    "    conv8_2 = conv_layer(conv8_1, shape = [3, 3, 256, 256], name='conv8_2', is_training = is_training)\n",
    "    residual_5 = tf.add(conv8_2, maxpool_3, name='residual_5')\n",
    "    \n",
    "    # Ninth block of conv layers\n",
    "    conv9_1 = conv_layer(residual_5, shape = [3, 3, 256, 256], name='conv9_1', is_training = is_training)\n",
    "    conv9_2 = conv_layer(conv9_1, shape = [3, 3, 256, 256], name='conv9_2', is_training = is_training)\n",
    "    residual_6 = tf.add(conv9_2, residual_5, name='residual_6')\n",
    "    \n",
    "    # Tenth block of conv layers\n",
    "    conv10_1 = conv_layer(residual_6, shape = [3, 3, 256, 256], name='conv10_1', is_training = is_training)\n",
    "    maxpool_4 = max_pool_layer(conv10_1, 3, 2)\n",
    "    \n",
    "    # First fully connected\n",
    "    fc_1 = fc_layer(maxpool_4, 1024, name='fc_1')\n",
    "\n",
    "    # Apply Dropout\n",
    "    drop = dropout_layer(fc_1, dropout)\n",
    "\n",
    "    # Second fully connected\n",
    "    fc_2 = fc_layer(drop, 100, name='fc_2', use_relu = False)\n",
    "  \n",
    "  return {\"output\": fc_2, \"deep_feature\": fc_1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "93ArYAK1VM5w"
   },
   "source": [
    "## Parte 4:\tDefinición de modelos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wm0ryn0s5Ydm"
   },
   "source": [
    "## Definimos el modelo para la red skNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "V8uBikYrRzrG"
   },
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        is_training = True\n",
    "    else:\n",
    "        is_training = False\n",
    "        \n",
    "    net = skNet(features, params['number_of_classes'], is_training=is_training)\n",
    "    train_net = net[\"output\"]\n",
    "    \n",
    "    idx_predicted_class = tf.argmax(train_net, 1)\n",
    "    # If prediction mode, \n",
    "    predictions = { \"idx_predicted_class\": idx_predicted_class,\n",
    "                    \"predicted_probabilities\": tf.nn.softmax(train_net, name=\"pred_probs\"),\n",
    "                    \"deep_feature\" : net[\"deep_feature\"]\n",
    "                   }\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        estim_specs = tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "    else : # TRAIN or EVAL\n",
    "        idx_true_class = tf.argmax(labels, 1)            \n",
    "        # Evaluate the accuracy of the model\n",
    "        acc_op = tf.metrics.accuracy(labels=idx_true_class, predictions=idx_predicted_class)    \n",
    "        # Define loss - e.g. cross_entropy -> mean(cross_entropy x batch)\n",
    "        #onehot_labels = tf.one_hot(tf.cast(labels, tf.int32), 100)\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits = train_net, labels = labels)\n",
    "        loss = tf.reduce_mean(cross_entropy)   \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) # in order to allow updating in batch_normalization\n",
    "        with tf.control_dependencies(update_ops) :\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate= learning_rate)\n",
    "            train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())        \n",
    "        #EstimatorSpec \n",
    "        estim_specs = tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions=idx_predicted_class,\n",
    "            loss=loss,\n",
    "            train_op=train_op,\n",
    "            eval_metric_ops={'accuracy': acc_op})    \n",
    "    \n",
    "    return  estim_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X0zo7dXr5jPK"
   },
   "source": [
    "## Definimos el modelo para la red skResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "gk6yxI7Z_Cjf"
   },
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        is_training = True\n",
    "    else:\n",
    "        is_training = False\n",
    "        \n",
    "    net = skResNet(features, params['number_of_classes'], is_training=is_training)\n",
    "    train_net = net[\"output\"]\n",
    "    \n",
    "    idx_predicted_class = tf.argmax(train_net, 1)\n",
    "    # If prediction mode, \n",
    "    predictions = { \"idx_predicted_class\": idx_predicted_class,\n",
    "                    \"predicted_probabilities\": tf.nn.softmax(train_net, name=\"pred_probs\"),\n",
    "                    \"deep_feature\" : net[\"deep_feature\"]\n",
    "                   }\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        estim_specs = tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "    else : # TRAIN or EVAL\n",
    "        idx_true_class = tf.argmax(labels, 1)            \n",
    "        # Evaluate the accuracy of the model\n",
    "        acc_op = tf.metrics.accuracy(labels=idx_true_class, predictions=idx_predicted_class)    \n",
    "        # Define loss - e.g. cross_entropy -> mean(cross_entropy x batch)\n",
    "        #onehot_labels = tf.one_hot(tf.cast(labels, tf.int32), 100)\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits = train_net, labels = labels)\n",
    "        loss = tf.reduce_mean(cross_entropy)   \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) # in order to allow updating in batch_normalization\n",
    "        with tf.control_dependencies(update_ops) :\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate= learning_rate)\n",
    "            train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())        \n",
    "        #EstimatorSpec \n",
    "        estim_specs = tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions=idx_predicted_class,\n",
    "            loss=loss,\n",
    "            train_op=train_op,\n",
    "            eval_metric_ops={'accuracy': acc_op})    \n",
    "    \n",
    "    return  estim_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z1VfrPr8TicR"
   },
   "source": [
    "## Esta función sirve para ambos modelos sin problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "SAchOYXMICZZ"
   },
   "outputs": [],
   "source": [
    "#define the input function\n",
    "def input_fn(filename, image_shape, mean_img, batch_size, nr_batches, epochs, is_training=True):     \n",
    "    dataset = tf.data.TFRecordDataset(filename)\n",
    "    dataset = dataset.map(lambda x: parser_tfrecord_sk(x, image_shape[0], mean_img, 100))\n",
    "    dataset = dataset.batch(batch_size)   \n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(nr_batches) #1000 for train, 50 for test -> dataset_size/batch_size\n",
    "        dataset = dataset.repeat(epochs)            \n",
    "    # for testing shuffle and repeat are not required    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ieWI14TuTm9z"
   },
   "source": [
    "## Esta celda sirve para ambos modelos sin problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "37HxmjUZK1ww"
   },
   "outputs": [],
   "source": [
    "#metadata\n",
    "#metadata\n",
    "filename_mean = '/content/t2/tfrecords/mean.dat'\n",
    "metadata_file = '/content/t2/tfrecords/metadata.dat'\n",
    "#reading metadata    \n",
    "metadata_array = np.fromfile(metadata_file, dtype=np.int)\n",
    "image_shape = metadata_array[0:2]    \n",
    "number_of_classes = metadata_array[2]\n",
    "print(metadata_array)\n",
    "#load mean\n",
    "mean_img =np.fromfile(filename_mean, dtype=np.float64)\n",
    "mean_img = np.reshape(mean_img, image_shape.tolist())    \n",
    "#defining files for training and test\n",
    "filename_train = '/content/t2/tfrecords/train.tfrecords'\n",
    "filename_test = '/content/t2/tfrecords/test.tfrecords'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RGDpFMqIT6R1"
   },
   "source": [
    "## La siguiente celda corresponde al entrenamiento de la red skNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "TyO_DHCET9-G"
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "  classifier = tf.estimator.Estimator(model_fn = model_fn,\n",
    "                                      params = {'learning_rate' : learning_rate,\n",
    "                                                'number_of_classes' : 100,\n",
    "                                                'image_shape' : image_shape,\n",
    "                                                }\n",
    "                                      )\n",
    "  tf.logging.set_verbosity(tf.logging.INFO)\n",
    "  train_spec = tf.estimator.TrainSpec(input_fn = lambda: input_fn(filename_train, \n",
    "                                                                  image_shape, \n",
    "                                                                  mean_img, \n",
    "                                                                  batch_size,\n",
    "                                                                  1000,\n",
    "                                                                  10,\n",
    "                                                                  is_training = True),\n",
    "                                     max_steps = 14000)\n",
    "  eval_spec = tf.estimator.EvalSpec(input_fn = lambda: input_fn(filename_test, \n",
    "                                                                image_shape, \n",
    "                                                                mean_img, \n",
    "                                                                batch_size,\n",
    "                                                                1000,\n",
    "                                                                10,\n",
    "                                                                is_training = False),\n",
    "                                    start_delay_secs = 60,\n",
    "                                    throttle_secs = 120)\n",
    "  tf.estimator.train_and_evaluate(classifier, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9wKGc51vTtV9"
   },
   "source": [
    "## La siguiente celda corresponde al entrenamiento de la red skResNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "trH8Hn1jNhfe"
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "  classifier = tf.estimator.Estimator(model_fn = model_fn,\n",
    "                                      params = {'learning_rate' : learning_rate,\n",
    "                                                'number_of_classes' : 100,\n",
    "                                                'image_shape' : image_shape,\n",
    "                                                }\n",
    "                                      )\n",
    "  tf.logging.set_verbosity(tf.logging.INFO)\n",
    "  train_spec = tf.estimator.TrainSpec(input_fn = lambda: input_fn(filename_train, \n",
    "                                                                  image_shape, \n",
    "                                                                  mean_img, \n",
    "                                                                  batch_size,\n",
    "                                                                  2000,\n",
    "                                                                  10,\n",
    "                                                                  is_training = True),\n",
    "                                     max_steps = 14000)\n",
    "  eval_spec = tf.estimator.EvalSpec(input_fn = lambda: input_fn(filename_test, \n",
    "                                                                image_shape, \n",
    "                                                                mean_img, \n",
    "                                                                batch_size,\n",
    "                                                                1000,\n",
    "                                                                10,\n",
    "                                                                is_training = False),\n",
    "                                    start_delay_secs = 60,\n",
    "                                    throttle_secs = 120)\n",
    "  tf.estimator.train_and_evaluate(classifier, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m7QnK64IUQF7"
   },
   "source": [
    "## La siguiente celda ejecuta el test de cualquiera de las dos redes, se puede usar sin problemas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "9fReEmCMQC1G"
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "  result = classifier.evaluate(input_fn=lambda: input_fn(filename_test, \n",
    "                                                       image_shape, \n",
    "                                                       mean_img, \n",
    "                                                       batch_size,\n",
    "                                                       50,\n",
    "                                                       1,\n",
    "                                                       is_training = False),\n",
    "                             )\n",
    "  print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G6Gs7QDbUgI9"
   },
   "source": [
    "## La siguiente celda imprime el Testing Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RI_U-xp2PYdB"
   },
   "outputs": [],
   "source": [
    "print(\"Testing Accuracy:\", result['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aaQ1ndIXVYe_"
   },
   "source": [
    "## Parte 5:\tObtención de features y cálculo de mAP (Mean Average Precision) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QOaGjiF5gxiq"
   },
   "source": [
    "## Obtenemos los vectores de features para las 5000 imagenes de test con el modelo skNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3TZEFUE-UmnR"
   },
   "source": [
    "Es importante tener la ruta en donde se tiene guardado el modelo y sus checkpoints, en este caso tiene una ruta en duro debido a que se estaba probando, pero para obtener los vectores se debe ingresar a mano la ruta donde se tenga el modelo entrenado en la variable \"model_dir\" en la función tf.estimator.Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "r-PeEcmEUjkI"
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "  classifier = tf.estimator.Estimator(model_fn = model_fn,\n",
    "                                      model_dir = '/tmp/tmp8u5rcn0t/model.ckpt-10259',\n",
    "                                      params = {'learning_rate' : 0,\n",
    "                                                'number_of_classes' : 100,\n",
    "                                                'image_shape' : image_shape\n",
    "                                                })\n",
    "  tf.logging.set_verbosity(tf.logging.INFO)\n",
    "  \n",
    "  predicted_result = list(classifier.predict(input_fn=lambda: input_fn(filename_test, \n",
    "                                                       image_shape, \n",
    "                                                       mean_img, \n",
    "                                                       batch_size,\n",
    "                                                       50,\n",
    "                                                       1,\n",
    "                                                       is_training = False)))\n",
    "  features = []\n",
    "  for prediction in predicted_result:\n",
    "    deep_features = prediction[\"deep_feature\"]\n",
    "    features.append(deep_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t-INdckkZ0E3"
   },
   "source": [
    "## Obtenemos los vectores de features para las 5000 imagenes de test con el modelo skResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oYdkiX3jU9Az"
   },
   "source": [
    "Es importante tener la ruta en donde se tiene guardado el modelo y sus checkpoints, en este caso tiene una ruta en duro debido a que se estaba probando, pero para obtener los vectores se debe ingresar a mano la ruta donde se tenga el modelo entrenado en la variable \"model_dir\" en la función tf.estimator.Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XEljAsGdZ2rt"
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "  classifier = tf.estimator.Estimator(model_fn = model_fn,\n",
    "                                      model_dir = '/tmp/tmpu6lii0fg/model.ckpt-14000',\n",
    "                                      params = {'learning_rate' : 0,\n",
    "                                                'number_of_classes' : 100,\n",
    "                                                'image_shape' : image_shape\n",
    "                                                })\n",
    "  tf.logging.set_verbosity(tf.logging.INFO)\n",
    "  \n",
    "  predicted_result = list(classifier.predict(input_fn=lambda: input_fn(filename_test, \n",
    "                                                       image_shape, \n",
    "                                                       mean_img, \n",
    "                                                       batch_size,\n",
    "                                                       50,\n",
    "                                                       1,\n",
    "                                                       is_training = False)))\n",
    "  features = []\n",
    "  for prediction in predicted_result:\n",
    "    deep_features = prediction[\"deep_feature\"]\n",
    "    features.append(deep_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ti0hrpiNVjA_"
   },
   "source": [
    "La siguiente celda se encarga de calcular el mAP del modelo que se esté probando en estos momentos. Es importante que para calcular el mAP, antes se haya ejecutado todas las celdas hacia arriba debido a que se usa el vector con las clases reales antes de que se transforme en TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "GbIpD24-pJTT"
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "import itertools\n",
    "l1 = features\n",
    "l2 = test_classes\n",
    "p = zip(l1, l2)\n",
    "i = 0\n",
    "mAP = 0\n",
    "aux = []\n",
    "for x, y in itertools.permutations(p, 2):\n",
    "  if (i % (len(l1) - 1) == 0) and (i != 0):\n",
    "    aux.sort()\n",
    "    true = 0\n",
    "    count = 0\n",
    "    ap = 0\n",
    "    for elem in aux:\n",
    "      count += 1\n",
    "      if var[1] == elem[1]:\n",
    "        true += 1\n",
    "        ap += float(true)/float(count)\n",
    "    if true > 0:\n",
    "      mAP += float(ap)/float(true)\n",
    "    del aux[:]\n",
    "  dist = distance.euclidean(x[0], y[0])\n",
    "  aux.append([dist, y[1]])\n",
    "  var = x\n",
    "  i += 1\n",
    "mAP = float(mAP)/float(len(l1))\n",
    "print('El mAP obtenido es: ')\n",
    "print(mAP)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Tarea2_Deep_Learning.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
